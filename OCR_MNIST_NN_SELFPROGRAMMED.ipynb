{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OCR MNIST DIGITS - Self programmed\n",
    "\n",
    "The objective of this notebook is to make an OCR with neural networks.\n",
    "This is exercise is for learning purpose, so do not expect an optimized solution.\n",
    "The data used shall be MNIST digits.\n",
    "\n",
    "The problem is obtained from:\n",
    "\n",
    "https://www.kaggle.com/c/digit-recognizer/data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "Data is in file \"train.csv\".\n",
    "\n",
    "First column: label y (from 0->9)\n",
    "\n",
    "Following 784 columns: pixels in order 28X28\n",
    "\n",
    "Pixels range value: 0..255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train=np.genfromtxt(\"train.csv\", unpack=True, skip_header=1,delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization of data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually in a Deep learning problem it is used: training data, cross-validation data and test data. Kaggle problem gives a specific set for test data that is used to check the full model.\n",
    "\n",
    "Note that usually (for example in Tensorflow) in data matrix, the data is ordered: (training sample,data). However, in this exercise, data shall be ordered (data,training sample)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full training data shape: (785, 42000)\n",
      "Training data:  38000\n",
      "Cross Validation data:  4000\n",
      "x training data shape: (784, 38000)\n",
      "Y training data shape: (10, 38000)\n",
      "x CV data shape: (784, 4000)\n",
      "Y CV data shape: (10, 4000)\n",
      "Number of training data: 38000\n",
      "Number of features: 784\n",
      "Images are: 28 x 28\n"
     ]
    }
   ],
   "source": [
    "print(\"Full training data shape: (%d, %d)\" % data_train.shape)\n",
    "\n",
    "# NC: number of categories\n",
    "NC=10\n",
    "\n",
    "# NP: number of pixels in the image\n",
    "NP=round((data_train.shape[0]-1)**0.5)\n",
    "\n",
    "# m: number of data images provided\n",
    "m=data_train.shape[1]\n",
    "\n",
    "# 90% training / 10% cross validation aprox.\n",
    "m_CV=4000\n",
    "m_train=m-m_CV\n",
    "\n",
    "print(\"Training data: \",m_train)\n",
    "print(\"Cross Validation data: \",m_CV)\n",
    "\n",
    "# Train data\n",
    "## Label\n",
    "### Sparse categorical\n",
    "y_train=data_train[0,0:m_train].reshape(1,-1)\n",
    "### One_hot categorical\n",
    "Y_train=np.zeros((10,y_train.shape[1]))\n",
    "Y_train[y_train.astype(int),range(y_train.shape[1])]=1\n",
    "## Image data\n",
    "xx_train=data_train[1:data_train.shape[0],0:m_train]/255\n",
    "### Around each digit there is a black frame which can be removed. Cut gives the number of pixels removed from the frame.\n",
    "cut=0\n",
    "x_train=((xx_train.T.reshape(m_train,NP,NP))[:,cut:NP-cut,cut:NP-cut].reshape(m_train,-1)).T\n",
    "\n",
    "# Train data\n",
    "## Label\n",
    "### Sparse categorical\n",
    "y_CV=data_train[0,m_train:].reshape(1,-1)\n",
    "### One_hot categorical\n",
    "Y_CV=np.zeros((10,y_CV.shape[1]))\n",
    "Y_CV[y_CV.astype(int),range(y_CV.shape[1])]=1\n",
    "## Image data\n",
    "xx_CV=data_train[1:data_train.shape[0],m_train:]/255\n",
    "x_CV=((xx_CV.T.reshape(m_CV,NP,NP))[:,cut:NP-cut,cut:NP-cut].reshape(m_CV,-1)).T\n",
    "\n",
    "print(\"x training data shape: (%d, %d)\" % x_train.shape)\n",
    "print(\"Y training data shape: (%d, %d)\" % Y_train.shape)\n",
    "print(\"x CV data shape: (%d, %d)\" % x_CV.shape)\n",
    "print(\"Y CV data shape: (%d, %d)\" % Y_CV.shape)\n",
    "print(\"Number of training data: %d\" % m_train)\n",
    "n_x=x_train.shape[0]\n",
    "print(\"Number of features: %d\" % n_x)\n",
    "NP=NP-2*cut\n",
    "print(\"Images are: %d x %d\" %(NP,NP))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Following picture is label as 3\n",
      "(28, 28)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "example2plot=1050\n",
    "print(\"Following picture is label as %d\" % y_train[0,example2plot])\n",
    "example=(x_train[:,example2plot].reshape(NP,NP))\n",
    "print(example.shape)\n",
    "imgplot = plt.imshow(example)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definition of activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(z):\n",
    "    return ((z>0)*z).reshape(z.shape[0],z.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_derivate(z):\n",
    "    return ((z>0)*1).reshape(z.shape[0],z.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return (1/(1+np.exp(-z))).reshape(z.shape[0],z.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_derivate(z):\n",
    "    aux=sigmoid(z)\n",
    "    return (aux*(1-aux)).reshape(z.shape[0],z.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    return (np.exp(z)/((np.exp(z)).sum(axis=0)).reshape(1,z.shape[1])).reshape(z.shape[0],z.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definition of the neural network parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 1: has 324 neurons of type relu\n",
      "Layer 2: has 324 neurons of type relu\n",
      "Layer 3: has 10 neurons of type softmax\n"
     ]
    }
   ],
   "source": [
    "# Number of layers\n",
    "L=3\n",
    "\n",
    "# Neurons per layer\n",
    "n_h=[324,324,10]\n",
    "\n",
    "# Type of activation at each layer\n",
    "typeN=[\"relu\",\"relu\",\"softmax\"]\n",
    "\n",
    "for i in range(L):\n",
    "    print(\"Layer %d: has %d neurons of type %s\" % (i+1, n_h[i], typeN[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN Model functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to initialize\n",
    "\n",
    "**For relu / linear:**\n",
    "\n",
    "In initialitation, the factor *np.sqrt(2/nn[l])* is applied to avoid canishing/exploding gradients in large deep neural networks.\n",
    "\n",
    "**For sigmoid / softmax:**\n",
    "\n",
    "In initialitation, the factor *np.sqrt(1/nn[l])* is applied to avoid canishing/exploding gradients in large deep neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_NN(L,n_x,n_h,typeN):\n",
    "    ## This function initialize the neural network creating a dictionary call parameters.\n",
    "    ## INPUTS\n",
    "    ##### L: number of layers\n",
    "    ##### n_x: length of input vector\n",
    "    ##### n_h: list with the number of neurons in each layer\n",
    "    ##### typeN: type of activation applied at each layer\n",
    "    ## OUTPUTS\n",
    "    ##### parameters dictionary, contains \"W_\" and \"b_\" of each layer.\n",
    "    nn=[n_x]+n_h\n",
    "    parameters={}\n",
    "    for l in range(L):\n",
    "        if ((typeN[l]==\"relu\")|(typeN[l]==\"linear\")):\n",
    "            parameters[\"W\"+str(l+1)]=np.random.randn(nn[l+1],nn[l])*np.sqrt(2/nn[l])\n",
    "        if ((typeN[l]==\"sigmoid\")|(typeN[l]==\"softmax\")):\n",
    "            parameters[\"W\"+str(l+1)]=np.random.randn(nn[l+1],nn[l])*np.sqrt(1/nn[l])\n",
    "        parameters[\"b\"+str(l+1)]=np.zeros((nn[l+1],1))\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feedforward functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feedforward_layer(parameters,typeN,l,A_prev):\n",
    "    ## feedforward of a layer\n",
    "    ## INPUTS:\n",
    "    ##### parameters: parameters of all the layers (\"W_\" and \"b_\")\n",
    "    ##### typeN: type of activation applied at each layer\n",
    "    ##### l: current layer that is going to be feedforwarded\n",
    "    ##### A_prev: activation from previous layer\n",
    "    ## OUTPUTS\n",
    "    ##### Z = W @ A_prev + b\n",
    "    ##### A = Activation(Z)\n",
    "    \n",
    "    m=A_prev.shape[1]\n",
    "    \n",
    "    W=parameters[\"W\"+str(l)]\n",
    "    b=parameters[\"b\"+str(l)]\n",
    "    \n",
    "    n_h=W.shape[0]\n",
    "    \n",
    "    Z=W@A_prev+b\n",
    "    \n",
    "    Z=Z.reshape(n_h,m)\n",
    "    \n",
    "    if typeN[l-1]==\"relu\":\n",
    "        A=relu(Z)\n",
    "    if typeN[l-1]==\"sigmoid\":\n",
    "        A=sigmoid(Z)\n",
    "    if typeN[l-1]==\"softmax\":\n",
    "        A=softmax(Z)\n",
    "    if typeN[l-1]==\"linear\":\n",
    "        A=np.copy(Z)\n",
    "    return A,Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feedforward_model(parameters,typeN,X):\n",
    "    ## feedforward the complete neural network\n",
    "    ## INPUTS\n",
    "    ##### parameters: parameters of all the layers (\"W_\" and \"b_\")\n",
    "    ##### typeN: type of activation applied at each layer\n",
    "    ##### X: input matrix: (data,training samples)\n",
    "    ## OUTPUTS\n",
    "    ##### Z dictionary with the values at every layer\n",
    "    ##### A dictionary with the value of activations at every layer: A[1],A[2]...A[L]\n",
    "    \n",
    "    L=len(parameters)//2\n",
    "    A={}\n",
    "    Z={}\n",
    "    \n",
    "    A_prev=X\n",
    "    for l in range(L):\n",
    "        A[l+1],Z[l+1] = feedforward_layer(parameters,typeN,l+1,A_prev)\n",
    "        A_prev=A[l+1]\n",
    "        \n",
    "    return A,Z\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back propagation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop_layer(parameters,typeN,l,A_lm1,Z_lm1,grad_l):\n",
    "    ## backprop of a layer: calculation of dW_l, db_l and gradient respect Z_(l-1) at l-1\n",
    "    ## INPUTS:\n",
    "    ##### parameters: parameters of all the layers (\"W_\" and \"b_\")\n",
    "    ##### typeN: type of activation applied at each layer\n",
    "    ##### l: current layer that is going to be feedforwarded\n",
    "    ##### A_lm1: activation from layer l-1\n",
    "    ##### Z_lm1: Z from layer l-1\n",
    "    ##### grad_l\n",
    "    ## OUTPUTS\n",
    "    ##### dW: dcost/dW at layer l\n",
    "    ##### db: dcost/db at layer l\n",
    "    ##### grad_lm1: dcost/dZ_(l-1) at layer l-1\n",
    "        \n",
    "    m=A_lm1.shape[1]\n",
    "    W=parameters[\"W\"+str(l)]\n",
    "    dW=(1/m*grad_l@A_lm1.T).reshape(W.shape[0],W.shape[1])\n",
    "    db=(1/m*np.sum(grad_l,axis=1,keepdims=True)).reshape(W.shape[0],1)\n",
    "    if l!=1:\n",
    "        grad_lm1=(W.T@grad_l)\n",
    "        if typeN[l-1]==\"relu\":\n",
    "            grad_lm1=grad_lm1*relu_derivate(Z_lm1)\n",
    "        if typeN[l-1]==\"sigmoid\":\n",
    "            grad_lm1=grad_lm1*sigmoid_derivate(Z_lm1)\n",
    "    else:\n",
    "        grad_lm1=0\n",
    "    return dW,db,grad_lm1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop_model(parameters,typeN,X,Y,A,Z):\n",
    "    ## backprop of a layer: calculation of dW_l, db_l and gradient respect Z_(l-1) at l-1\n",
    "    ## INPUTS:\n",
    "    ##### parameters: parameters of all the layers (\"W_\" and \"b_\")\n",
    "    ##### typeN: type of activation applied at each layer\n",
    "    ##### X: input matrix: (data,training samples)\n",
    "    ##### Y: label in one-hot categorical (label,training samples)\n",
    "    ##### A dictionary with the value of activations at every layer: A[1],A[2]...A[L]\n",
    "    ##### Z dictionary with the values at every layer\n",
    "    ## OUTPUTS\n",
    "    ##### grads: dictionary containing for each layer [dW_] and [db_]\n",
    "    L=len(parameters)//2\n",
    "    ## dcost/dZ_l for last layer is always A[L]-Y independently of activation.\n",
    "    ## This was checked for relu, linear, sigmoid and softmax, with cross entropy loss\n",
    "    grad_l=A[L]-Y\n",
    "    A[0]=X\n",
    "    Z[0]=X\n",
    "    grads={}\n",
    "    for l in range(L,0,-1):\n",
    "        grads[\"dW\"+str(l)], grads[\"db\"+str(l)], grad_l=backprop_layer(parameters,typeN,l,A[l-1],Z[l-1],grad_l)  \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RegularizationL2(parameters,grads,lambda_regularization,m):\n",
    "    ## Add L2 regularization to grads\n",
    "    ## INPUTS:\n",
    "    ##### parameters: parameters of all the layers (\"W_\" and \"b_\")\n",
    "    ##### grads: dictionary containing for each layer [dW_] and [db_]\n",
    "    ##### lambda_regularization: regularization parameter lambda\n",
    "    ##### m: number of training samples\n",
    "    ## OUTPUTS\n",
    "    ##### grads corrected with L2 regularization\n",
    "    L=len(parameters)//2\n",
    "    for l in range(L):\n",
    "        grads[\"dW\"+str(l+1)]=grads[\"dW\"+str(l+1)]+parameters[\"W\"+str(l+1)]*lambda_regularization/m\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_cost(Y_estimated,Y):\n",
    "    m=Y.shape[1]\n",
    "    cost =np.sum(-1/m*(Y*np.log(Y_estimated)+(1-Y)*np.log(1-Y_estimated)))\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_cost(Y_estimated,Y):\n",
    "    m=Y.shape[1]\n",
    "    cost =np.sum(-1/m*(Y*np.log(Y_estimated)))\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_cost(Y_estimated,Y):\n",
    "    m=Y.shape[1]\n",
    "    cost =np.sum(1/2/m*(Y-Y_estimated)**2)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(typeN):\n",
    "    if typeN[-1]==\"sigmoid\":\n",
    "        def loss(Y_estimated,Y):\n",
    "            m=Y.shape[1]\n",
    "            cost =np.sum(-1/m*(Y*np.log(Y_estimated)+(1-Y)*np.log(1-Y_estimated)))\n",
    "            return cost\n",
    "    if typeN[-1]==\"softmax\":\n",
    "        def loss(Y_estimated,Y):\n",
    "            m=Y.shape[1]\n",
    "            cost =np.sum(-1/m*(Y*np.log(Y_estimated)))\n",
    "            return cost\n",
    "    if typeN[-1]==\"linear\":\n",
    "        def loss(Y_estimated,Y):\n",
    "            m=Y.shape[1]\n",
    "            cost =np.sum(1/2/m*(Y-Y_estimated)**2)\n",
    "            return cost\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RegularizationL2_cost(parameters,lambda_regularization,m):\n",
    "    cost=0\n",
    "    for l in range(len(parameters)//2):\n",
    "        cost=cost+np.sum(parameters[\"W\"+str(l+1)]**2*lambda_regularization/m/2)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters,grads,learning_rate):\n",
    "    ## Update parameters after a feedforward-backprop\n",
    "    ## INPUTS:\n",
    "    ##### parameters: parameters of all the layers (\"W_\" and \"b_\")\n",
    "    ##### grads: dictionary containing for each layer [dW_] and [db_]\n",
    "    ##### learning_rate: learning rate\n",
    "    ## OUTPUTS\n",
    "    ##### parameters: parameters of all the layers (\"W_\" and \"b_\")\n",
    "    L=len(parameters)//2\n",
    "    for l in range(L):\n",
    "        parameters[\"W\"+str(l+1)]=parameters[\"W\"+str(l+1)]-grads[\"dW\"+str(l+1)]*learning_rate\n",
    "        parameters[\"b\"+str(l+1)]=parameters[\"b\"+str(l+1)]-grads[\"db\"+str(l+1)]*learning_rate\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction & Accuracy functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(parameters,typeN,X):\n",
    "    ## Predict category\n",
    "    ## INPUTS:\n",
    "    ##### parameters: parameters of all the layers (\"W_\" and \"b_\")\n",
    "    ##### typeN: type of activation applied at each layer\n",
    "    ##### X: input matrix: (data,training samples)\n",
    "    ## OUTPUTS\n",
    "    ##### y: sparse categorical (prediction,training samples)\n",
    "    A,Z = feedforward_model(parameters,typeN,X)\n",
    "    Y = A[len(parameters)//2]\n",
    "    ypred = Y.argmax(axis=0)\n",
    "    ypred = ypred.reshape(1,-1)\n",
    "    return ypred\n",
    "\n",
    "def evaluate(parameters,typeN,X):\n",
    "    ## evaluate across NN\n",
    "    ## INPUTS:\n",
    "    ##### parameters: parameters of all the layers (\"W_\" and \"b_\")\n",
    "    ##### typeN: type of activation applied at each layer\n",
    "    ##### X: input matrix: (data,training samples)\n",
    "    ## OUTPUTS\n",
    "    ##### y: probability ditribution of categories (prob.distribution,training samples)\n",
    "    A,Z = feedforward_model(parameters,typeN,X)\n",
    "    Y = A[len(parameters)//2]\n",
    "    return Y\n",
    "\n",
    "def accuracy(y_pred,y_test):\n",
    "    ## Checks accuracy of prediction\n",
    "    ## INPUTS:\n",
    "    ##### y_pred: prediction sparse categorical (prediction,training samples)\n",
    "    ##### y_test: real value sparse categorical (prediction,training samples)\n",
    "    ## OUTPUTS\n",
    "    ##### acc: accuracy\n",
    "    acc=((y_pred==y_test).sum())/y_test.shape[1]\n",
    "    return acc\n",
    "\n",
    "def Y2y(Y):\n",
    "    ## Gets Y in categorical, transform it to one-hot categorical and finally outputs y sparse categorical.\n",
    "    y=Y.argmax(axis=0)\n",
    "    y=y.reshape(1,-1)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solver functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exponentially Weighted average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EWA(gradnew,gradold,n_x,n_h,beta1=0.9):\n",
    "    ## Exponentially weighted average of grad.\n",
    "    ## INPUTS:\n",
    "    ##### gradnew: grad calculated in current feedforward - backprop iteration\n",
    "    ##### gradold: grad calculated in previous iteration\n",
    "    ##### n_x: length of input vector\n",
    "    ##### n_h: list with the number of neurons in each layer\n",
    "    ##### beta1: parameter to weight average, normally 0.9.\n",
    "    ## OUTPUTS\n",
    "    ##### gradold: grad updated according to: (beta1*gradold+(1-beta1)*gradnew)\n",
    "    L=len(gradnew)//2\n",
    "    for l in range(L):\n",
    "        gradold[\"dW\"+str(l+1)]=(beta1*gradold[\"dW\"+str(l+1)]+(1-beta1)*gradnew[\"dW\"+str(l+1)])\n",
    "        gradold[\"db\"+str(l+1)]=(beta1*gradold[\"db\"+str(l+1)]+(1-beta1)*gradnew[\"db\"+str(l+1)])\n",
    "    return gradold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_grad(L,n_x,n_h,bias=0):\n",
    "    ## Initialize grad to zero. Only needed if EWA is going to be used.\n",
    "    ## INPUTS:\n",
    "    ##### L: number of layers\n",
    "    ##### n_x: length of input vector\n",
    "    ##### n_h: list with the number of neurons in each layer\n",
    "    ##### bias: to initialize to a value different than zero\n",
    "    ## OUTPUTS\n",
    "    ##### grads: dictionary containing for each layer [dW_] and [db_]\n",
    "    nn=[n_x]+n_h\n",
    "    grads={}\n",
    "    for l in range(L):\n",
    "        grads[\"dW\"+str(l+1)]=np.zeros((nn[l+1],nn[l]))+bias\n",
    "        grads[\"db\"+str(l+1)]=np.zeros((nn[l+1],1))+bias\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for ADAM solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RMS(gradnew,S_old,n_x,n_h,beta2=0.999):\n",
    "    ## This function calculates square of grad (S=grad**2) and does an EWA with previous value\n",
    "    ## INPUTS:\n",
    "    ##### gradnew: grad calculated in current feedforward - backprop iteration\n",
    "    ##### S_old: grad squared calculated in previous iteration\n",
    "    ##### n_x: length of input vector\n",
    "    ##### n_h: list with the number of neurons in each layer\n",
    "    ##### beta2: parameter to weight average, normally 0.999.\n",
    "    ## OUTPUTS\n",
    "    ##### aux: grad squared updated according to: (beta2*S_old+(1-beta1)*gradnew**2)\n",
    "    L=len(gradnew)//2\n",
    "    aux={}\n",
    "    for l in range(L):\n",
    "        aux[\"dW\"+str(l+1)]=(beta2*S_old[\"dW\"+str(l+1)]+(1-beta2)*gradnew[\"dW\"+str(l+1)]**2)\n",
    "        aux[\"db\"+str(l+1)]=(beta2*S_old[\"db\"+str(l+1)]+(1-beta2)*gradnew[\"db\"+str(l+1)]**2)\n",
    "    return aux\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ADAM_grad(grads,V,S,n_x,n_h,beta1=0.9,beta2=0.999,counter=1e6,epsilon=1E-8):\n",
    "    ## This function corrects grad according to ADAM algorithm\n",
    "    ## INPUTS:\n",
    "    ##### grads: grad calculated in current feedforward - backprop iteration\n",
    "    ##### V: grad calculated in previous iteration without RMS correction\n",
    "    ##### S: grad squared calculated in previous iteration\n",
    "    ##### n_x: length of input vector\n",
    "    ##### n_h: list with the number of neurons in each layer\n",
    "    ##### beta1: parameter to weight average in EWA, normally 0.9.\n",
    "    ##### beta2: parameter to weight average in RMS, normally 0.999.\n",
    "    ##### counter: number of ADAM runs\n",
    "    ##### epsilon: parameter to avoid RMS vanishing.\n",
    "    ## OUTPUTS\n",
    "    ##### grads: grad calculated in current feedforward - backprop iteration updated with ADAM\n",
    "    ##### V: V updated\n",
    "    ##### S: S updated\n",
    "    ##### counter: counter updated\n",
    "    L=len(grads)//2\n",
    "    V=EWA(grads,V,n_x,n_h,beta1)\n",
    "    S=RMS(grads,S,n_x,n_h,beta2)\n",
    "    for l in range(L):\n",
    "        grads[\"dW\"+str(l+1)]=V[\"dW\"+str(l+1)]/(np.sqrt(S[\"dW\"+str(l+1)]/(1-beta2**counter))+epsilon)/(1-beta1**counter)\n",
    "        grads[\"db\"+str(l+1)]=V[\"db\"+str(l+1)]/(np.sqrt(S[\"db\"+str(l+1)]/(1-beta2**counter))+epsilon)/(1-beta1**counter)\n",
    "    counter=counter+1\n",
    "    return grads,V,S,counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model function\n",
    "\n",
    "The model includes:\n",
    "\n",
    "**Reduction on plateau of learning rate**\n",
    "\n",
    "Controlled with:\n",
    "\n",
    "*learning_rate_reduction*\n",
    "\n",
    "*min_learning_rate*\n",
    "\n",
    "*learning_rate_patience*\n",
    "\n",
    "**early stopping**\n",
    "\n",
    "Early stopping applied in cost_CV. Controlled with:\n",
    "\n",
    "*early_stopping* (patience)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#modelo\n",
    "def model(x_train,Y_train,x_CV,Y_CV,L,n_h,typeN,\n",
    "               solver_parameters,iterations=1):\n",
    "    ## Function to run the model\n",
    "    ## INPUTS:\n",
    "    ##### x_train: input matrix: (data,training samples)\n",
    "    ##### Y_train: categorical one-hot (one-hot labels,training samples)\n",
    "    ##### x_CV: x data for cross-validation\n",
    "    ##### Y_CV: Y data for cross-validation\n",
    "    ##### L: number of layers\n",
    "    ##### n_h: list with the number of neurons in each layer\n",
    "    ##### typeN: type of activation applied at each layer\n",
    "    ##### solver_parameters[\"lambda_regularization\"]: lambda regularization parameter, default to 0\n",
    "    ##### solver_parameters[\"learning_rate\"]:learning rate, default to 0.1\n",
    "    ##### solver_parameters[\"learning_rate_reduction\"]:learning rate reduction factor, default to 1\n",
    "    ##### solver_parameters[\"min_learning_rate\"]:minimum learning rate, default to 0\n",
    "    ##### solver_parameters[\"learning_rate_patience\"]:learning rate patience to apply reduction, default to 1e6\n",
    "    ##### solver_parameters[\"b\"]: batch size, mandatory to define unless type = \"GradientDescent\"\n",
    "    ##### solver_parameters[\"early_stopping\"]: patience before stopping, default no early stopping\n",
    "    ##### solver_parameters[\"beta1\"]: beta parameters for EWA, default to 0.9\n",
    "    ##### solver_parameters[\"beta2\"]: beta parameters for RMS, default to 0.999\n",
    "    ##### solver_parameters[\"Type\"]=\"ADAM\", \"GradientDescent\",\"GradientDescentMiniBatch\",\"GradientDescentMomentum\". Mandatory to define.\n",
    "    ##### iterations: number of iterations\n",
    "\n",
    "    ## OUTPUTS\n",
    "    ##### parameters_best: best parameters for CV after optimization\n",
    "    ##### iteration_best: iteration number at which parameters are the best for CV\n",
    "    ##### cost: list with cost in training\n",
    "    ##### acc: list with accuracy in training\n",
    "    ##### cost_CV: list with cost in cross validation\n",
    "    ##### acc_CV: list with accuracy in cross validation\n",
    "    ##### parameters: parameters at last iteration\n",
    "    ##### grads: grads at last iteration\n",
    "    ##### S: S at last iteration\n",
    "    ##### V: V at last iteration\n",
    "    \n",
    "    # Input length\n",
    "    n_x=x_train.shape[0]\n",
    "    \n",
    "    # number of training samples\n",
    "    m_train=x_train.shape[1]\n",
    "    \n",
    "    # number of CV samples\n",
    "    m_CV=x_CV.shape[1]\n",
    "    \n",
    "    # NN parameters initialization\n",
    "    parameters=initialize_NN(L,n_x,n_h,typeN)\n",
    "    \n",
    "    ## Parameters for early stopping\n",
    "    early_stopping_flag=0 # Counter the number of iterations since last best parameters value\n",
    "    cost_best=1e10        # Best cost parameter\n",
    "    \n",
    "    ## Parameters for learning rate reduction\n",
    "    LR_flag=0 # Counter the number of iterations since last best parameters value\n",
    "    \n",
    "    ## Initialization of list for cost and acc \n",
    "    cost=[]\n",
    "    cost_CV=[]\n",
    "    acc=[]\n",
    "    acc_CV=[]\n",
    "    \n",
    "    # Creates a function for loss according to cross entropy with the type of activation in last layer\n",
    "    loss=loss_function(typeN)    \n",
    "    \n",
    "    \n",
    "    # Extraction of parameters from solver_parameters\n",
    "    if \"learning_rate\" in solver_parameters.keys():\n",
    "        learning_rate=solver_parameters[\"learning_rate\"]\n",
    "    else:\n",
    "        learning_rate=0.1\n",
    "    learning_rate_used=learning_rate       \n",
    "    \n",
    "    if \"lambda_regularization\" in solver_parameters.keys():\n",
    "        lambda_regularization=solver_parameters[\"lambda_regularization\"]\n",
    "    else:\n",
    "        lambda_regularization=0      \n",
    "    \n",
    "    if \"early_stopping\" in solver_parameters.keys():\n",
    "        early_stopping=solver_parameters[\"early_stopping\"]\n",
    "    else:\n",
    "        early_stopping=-1\n",
    " \n",
    "    if \"learning_rate_reduction\" in solver_parameters.keys():\n",
    "        learning_rate_reduction=solver_parameters[\"learning_rate_reduction\"]\n",
    "    else:\n",
    "        learning_rate_reduction=1 \n",
    "        \n",
    "    if \"min_learning_rate\" in solver_parameters.keys():\n",
    "        min_learning_rate=solver_parameters[\"min_learning_rate\"]\n",
    "    else:\n",
    "        min_learning_rate=0 \n",
    "        \n",
    "    if \"learning_rate_patience\" in solver_parameters.keys():\n",
    "        learning_rate_patience=solver_parameters[\"learning_rate_patience\"]\n",
    "    else:\n",
    "        learning_rate_patience=1e6 \n",
    "        \n",
    "    # Solver initialization\n",
    "    if solver_parameters[\"Type\"] == \"GradientDescent\":\n",
    "        b=m_train\n",
    "    \n",
    "    elif solver_parameters[\"Type\"] == \"GradientDescentMiniBatch\":\n",
    "        b=solver_parameters[\"b\"]\n",
    "        if \"beta1\" in solver_parameters.keys():\n",
    "            beta1=solver_parameters[\"beta1\"]\n",
    "        else:\n",
    "            beta1=0.9\n",
    "    elif solver_parameters[\"Type\"] == \"GradientDescentMomentum\":\n",
    "        b=solver_parameters[\"b\"]\n",
    "        if \"beta1\" in solver_parameters.keys():\n",
    "            beta1=solver_parameters[\"beta1\"]\n",
    "        else:\n",
    "            beta1=0.9\n",
    "        V=initialize_grad(L,n_x,n_h)\n",
    "        \n",
    "    elif solver_parameters[\"Type\"] == \"ADAM\":\n",
    "        b=solver_parameters[\"b\"]\n",
    "        if \"beta1\" in solver_parameters.keys():\n",
    "            beta1=solver_parameters[\"beta1\"]\n",
    "        else:\n",
    "            beta1=0.9\n",
    "        if \"beta2\" in solver_parameters.keys():\n",
    "            beta2=solver_parameters[\"beta2\"]\n",
    "        else:\n",
    "            beta2=0.999\n",
    "        V=initialize_grad(L,n_x,n_h)\n",
    "        S=initialize_grad(L,n_x,n_h,1)\n",
    "        counter=1\n",
    "     \n",
    "    \n",
    "    for i in range(iterations):\n",
    "        for j in range(int(np.ceil(m_train/b))):\n",
    "            xx=x_train[:,(j)*b:(j+1)*b] ## Data for current batch\n",
    "            yy=Y_train[:,(j)*b:(j+1)*b] ## Label for current batch\n",
    "            b_aux=xx.shape[1]           ## number of samples of current batch, can be lower than b for last batch\n",
    "            \n",
    "            # Feedforward propagation\n",
    "            A,Z=feedforward_model(parameters,typeN,xx)\n",
    "            \n",
    "            # Calculation of cost function\n",
    "            costs=loss(A[L],yy)+RegularizationL2_cost(parameters,lambda_regularization,b)\n",
    "            \n",
    "            # Calculation of accuracy\n",
    "            accs=accuracy(Y2y(A[L]),Y2y(yy))\n",
    "         \n",
    "            # Backprop\n",
    "            grads=backprop_model(parameters,typeN,xx,yy,A,Z)\n",
    "            \n",
    "            # Addition of L2 regularization\n",
    "            grads=RegularizationL2(parameters,grads,lambda_regularization,b)\n",
    "            \n",
    "            # Correction of cost and accs for non full batch methods\n",
    "            if solver_parameters[\"Type\"] != \"GradientDescent\":\n",
    "                if ((i>0)&(j>0)):\n",
    "                    costs=beta1*cost_old+(1-beta1)*costs\n",
    "                    accs=beta1*acc_old+(1-beta1)*accs\n",
    "                cost_old=costs\n",
    "                acc_old=accs \n",
    "               \n",
    "            # Correction of gradient according to selected method:\n",
    "            if solver_parameters[\"Type\"] == \"GradientDescentMomentum\":\n",
    "                grads=EWA(grads,V,n_x,n_h,beta1)\n",
    "                V=grads\n",
    "                \n",
    "            elif solver_parameters[\"Type\"] == \"ADAM\":\n",
    "                grads,V,S,counter=ADAM_grad(grads,V,S,n_x,n_h,beta1,beta2,counter)\n",
    "            \n",
    "            # Update parameters\n",
    "            parameters=update_parameters(parameters,grads,learning_rate_used)\n",
    "\n",
    "        # Append cost and accuracy\n",
    "        cost.append(costs)\n",
    "        acc.append(accs)\n",
    "        \n",
    "        # CV prediction\n",
    "        YPred_CV=evaluate(parameters,typeN,x_CV)\n",
    "        cost_CVs=loss(YPred_CV,Y_CV)+RegularizationL2_cost(parameters,lambda_regularization,b)\n",
    "        cost_CV.append(cost_CVs)\n",
    "        acc_CVs=accuracy(Y2y(YPred_CV),Y2y(Y_CV))\n",
    "        acc_CV.append(acc_CVs)\n",
    "        \n",
    "        # Print results\n",
    "        if i%10==0:\n",
    "            print(\"iteration: %d // cost_train: %f // acc_train: %f // cost_CV: %f // acc_CV: %f\" % (i, costs, accs,cost_CVs,acc_CVs))\n",
    "\n",
    "        # Learning rate reduction\n",
    "        if cost_best>cost_CVs:\n",
    "            LR_flag=0\n",
    "        else:\n",
    "            LR_flag = LR_flag + 1\n",
    "            if LR_flag>=learning_rate_patience:\n",
    "                learning_rate_used=max(learning_rate_used*learning_rate_reduction,min_learning_rate)\n",
    "                LR_flag=0\n",
    "                print(\"New learning rate: %f at iteration %i\" %(learning_rate_used,i))\n",
    "            \n",
    "        # Store of best solution & early stopping\n",
    "        if cost_best>cost_CVs:\n",
    "            parameters_best=parameters\n",
    "            iteration_best=i\n",
    "            cost_best=cost_CVs\n",
    "            early_stopping_flag=0\n",
    "        else:\n",
    "            if early_stopping>0:\n",
    "                early_stopping_flag = early_stopping_flag + 1\n",
    "                if early_stopping_flag>=early_stopping:\n",
    "                    break\n",
    "            \n",
    "    return parameters_best,iteration_best, cost, acc, cost_CV, acc_CV,parameters,grads,S,V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 0 // cost_train: 2.624391 // acc_train: 0.464286 // cost_CV: 2.583436 // acc_CV: 0.512500\n",
      "iteration: 10 // cost_train: 0.937352 // acc_train: 0.912111 // cost_CV: 0.943206 // acc_CV: 0.913250\n",
      "iteration: 20 // cost_train: 0.802188 // acc_train: 0.945056 // cost_CV: 0.828412 // acc_CV: 0.938000\n",
      "iteration: 30 // cost_train: 0.696778 // acc_train: 0.963644 // cost_CV: 0.731528 // acc_CV: 0.950750\n",
      "iteration: 40 // cost_train: 0.605751 // acc_train: 0.974213 // cost_CV: 0.646069 // acc_CV: 0.960500\n",
      "iteration: 50 // cost_train: 0.521953 // acc_train: 0.982514 // cost_CV: 0.567071 // acc_CV: 0.965750\n",
      "iteration: 60 // cost_train: 0.443036 // acc_train: 0.988304 // cost_CV: 0.493076 // acc_CV: 0.968750\n",
      "iteration: 70 // cost_train: 0.369733 // acc_train: 0.992561 // cost_CV: 0.423820 // acc_CV: 0.972000\n",
      "iteration: 80 // cost_train: 0.303671 // acc_train: 0.994574 // cost_CV: 0.360595 // acc_CV: 0.972750\n",
      "iteration: 90 // cost_train: 0.246754 // acc_train: 0.996291 // cost_CV: 0.304933 // acc_CV: 0.975000\n",
      "iteration: 100 // cost_train: 0.200629 // acc_train: 0.997130 // cost_CV: 0.259530 // acc_CV: 0.976000\n",
      "iteration: 110 // cost_train: 0.166084 // acc_train: 0.997633 // cost_CV: 0.225337 // acc_CV: 0.976500\n",
      "iteration: 120 // cost_train: 0.142318 // acc_train: 0.998058 // cost_CV: 0.201616 // acc_CV: 0.977500\n",
      "iteration: 130 // cost_train: 0.127388 // acc_train: 0.998172 // cost_CV: 0.186598 // acc_CV: 0.977250\n",
      "iteration: 140 // cost_train: 0.118801 // acc_train: 0.998353 // cost_CV: 0.177224 // acc_CV: 0.977500\n",
      "New learning rate: 0.050000 at iteration 146\n",
      "iteration: 150 // cost_train: 0.114596 // acc_train: 0.997947 // cost_CV: 0.174414 // acc_CV: 0.976750\n",
      "iteration: 160 // cost_train: 0.111851 // acc_train: 0.998147 // cost_CV: 0.171639 // acc_CV: 0.976500\n",
      "iteration: 170 // cost_train: 0.109763 // acc_train: 0.998574 // cost_CV: 0.168006 // acc_CV: 0.978250\n",
      "iteration: 180 // cost_train: 0.108422 // acc_train: 0.998734 // cost_CV: 0.166452 // acc_CV: 0.977750\n",
      "New learning rate: 0.025000 at iteration 182\n",
      "iteration: 190 // cost_train: 0.106531 // acc_train: 0.998561 // cost_CV: 0.161202 // acc_CV: 0.976750\n",
      "iteration: 200 // cost_train: 0.105484 // acc_train: 0.998442 // cost_CV: 0.159503 // acc_CV: 0.977750\n",
      "New learning rate: 0.012500 at iteration 207\n",
      "iteration: 210 // cost_train: 0.105306 // acc_train: 0.998329 // cost_CV: 0.161112 // acc_CV: 0.975500\n",
      "New learning rate: 0.006250 at iteration 210\n",
      "iteration: 220 // cost_train: 0.103258 // acc_train: 0.998951 // cost_CV: 0.157882 // acc_CV: 0.977250\n",
      "iteration: 230 // cost_train: 0.102787 // acc_train: 0.998816 // cost_CV: 0.156919 // acc_CV: 0.977000\n",
      "iteration: 240 // cost_train: 0.102543 // acc_train: 0.998922 // cost_CV: 0.156125 // acc_CV: 0.977000\n",
      "New learning rate: 0.003125 at iteration 248\n",
      "iteration: 250 // cost_train: 0.102112 // acc_train: 0.999014 // cost_CV: 0.157067 // acc_CV: 0.977000\n",
      "New learning rate: 0.001563 at iteration 251\n",
      "New learning rate: 0.000781 at iteration 254\n"
     ]
    }
   ],
   "source": [
    "solver_parameters={}\n",
    "solver_parameters[\"lambda_regularization\"]=1\n",
    "solver_parameters[\"learning_rate\"]=0.1\n",
    "solver_parameters[\"learning_rate_reduction\"]=0.5\n",
    "solver_parameters[\"min_learning_rate\"]=0.0001\n",
    "solver_parameters[\"learning_rate_patience\"]=3\n",
    "solver_parameters[\"b\"]=1024\n",
    "solver_parameters[\"early_stopping\"]=10\n",
    "solver_parameters[\"beta1\"]=0.9\n",
    "solver_parameters[\"beta2\"]=0.999\n",
    "solver_parameters[\"Type\"]=\"ADAM\"\n",
    "iterations=20000\n",
    "parameters,iteration_best, cost, acc, cost_CV, acc_CV,par,grads,S,V=model(x_train,Y_train,x_CV,Y_CV,L,n_h,typeN,\n",
    "                                                                     solver_parameters,iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "x=np.linspace(0,len(acc_CV),num=len(acc))\n",
    "line1, = ax.plot(x, np.array(acc),label='Acc train')\n",
    "x=np.arange(len(acc_CV))\n",
    "line2, = ax.plot(x, np.array(acc_CV),label='Acc CV')\n",
    "ax.legend()\n",
    "ax.set(xlim=(0, len(acc_CV)), ylim=(0.9, 1))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO3deXxU9b3/8ddnJjNJgBC2sBmWAG5EViPFsohLq9h6kdZWqNVat6q399rbVq+t/qq3v6v19ra9Vq1acav+rNi63Nq61bXWjRKQHZFdwhrCkoRss3x/f5xJCCGBkAROTvJ+Ph7zmDPnnDnz+Tr4nm++851zzDmHiIgEX8jvAkREpG0o0EVEOggFuohIB6FAFxHpIBToIiIdRJpfL9ynTx83dOhQv15eRCSQFixYsNM5l9PYNt8CfejQoRQWFvr18iIigWRmG5vapiEXEZEOQoEuItJBKNBFRDoI38bQRaTjicViFBUVUVVV5XcpgZeRkUFubi6RSKTZz1Ggi0ibKSoqIisri6FDh2JmfpcTWM45SkpKKCoqIi8vr9nP05CLiLSZqqoqevfurTBvJTOjd+/eR/yXjgJdRNqUwrxttOS/Y+ACfdW2Mn7511WUlFf7XYqISLsSuEBfW1zOvW+toViBLiKN2LZtG7NmzWL48OGMHDmS888/n08//fSIj3PnnXe2aNuhXHXVVaxYsaJFz22OwAV6RsQruSqW9LkSEWlvnHPMnDmTadOmsXbtWlasWMGdd97J9u3bj/hYLQl05xzJZNPZ9PDDDzNy5MgjrqW5AhfoWTU7OSe0gFjFXr9LEZF25u233yYSiXDttdfWrRs7dixTpkzBOceNN97IKaecwqhRo3jmmWcA2Lp1K1OnTmXs2LGccsop/P3vf+fmm2+msrKSsWPHcskllxzwGg23bdiwgZNPPpnrr7+e8ePHs2nTJq677joKCgrIz8/ntttuq3vutGnT6k550q1bN2655RbGjBnDxIkTW/Sh01Dgpi32KlnAw9FfUrj3bGCI3+WISBP+48/LWbGltE2POXJgd267IL/J7cuWLePUU09tdNvzzz/PokWLWLx4MTt37uS0005j6tSp/P73v+fcc8/llltuIZFIUFFRwZQpU7jvvvtYtGjRQce56667Dti2YcMGVq1axWOPPcb9998PwB133EGvXr1IJBKcffbZLFmyhNGjRx9wnH379jFx4kTuuOMObrrpJubMmcOtt97a0v80QAB76GnRTABiNRpDF5Hme++995g9ezbhcJh+/fpxxhlnMH/+fE477TQee+wxbr/9dpYuXUpWVtYRH3vIkCFMnDix7vEf/vAHxo8fz7hx41i+fHmj4+bRaJQvf/nLAJx66qls2LChxW2rFbgeelokCkA8pkAXac8O1ZM+WvLz83n22Wcb3eaca3T91KlTeffdd3nppZe49NJLufHGG7nsssuO6HW7du1at7x+/Xp+8YtfMH/+fHr27Mnll1/e6HzySCRSNzUxHA4Tj8eP6DUbE8AeegYACQW6iDRw1llnUV1dzZw5c+rWzZ8/n7/97W9MnTqVZ555hkQiQXFxMe+++y4TJkxg48aN9O3bl6uvvporr7yShQsXAl7gxmKxRl/nUNtKS0vp2rUr2dnZbN++nVdeeaXtG9qEwwa6mQ0ys7fNbKWZLTezGxrZZ5qZ7TWzRanbT45OuRCNpgMKdBE5mJnxwgsv8PrrrzN8+HDy8/O5/fbbGThwIDNnzmT06NGMGTOGs846i5///Of079+fd955h7FjxzJu3Diee+45brjBi7hrrrmG0aNHH/Sl6OG2jRkzhnHjxpGfn88VV1zBpEmTjnq7a1lTf4bU7WA2ABjgnFtoZlnAAuBC59yKevtMA37onPtyc1+4oKDAteQCF+Vr59HtyS/y2th7OffCI/uzSESOrpUrV3LyySf7XUaH0dh/TzNb4JwraGz/w/bQnXNbnXMLU8tlwErguDaotUUiqR56MqazuYmI1HdEY+hmNhQYB8xrZPPpZrbYzF4xs0a/DTGza8ys0MwKi4uLj7hYgGh6baDXtOj5IiIdVbMD3cy6Ac8B33PONZxcuhAY4pwbA9wL/G9jx3DOPeScK3DOFeTkNHqN08PXkZYK9LgCXUSkvmYFuplF8ML8Kefc8w23O+dKnXPlqeWXgYiZ9WnTSmuFvWmLCnQRkQM1Z5aLAY8AK51zv2pin/6p/TCzCanjlrRloXVSgU5cs1xEROprzg+LJgGXAkvNrPZ3sD8GBgM45x4ELgKuM7M4UAnMcoebPtNSYe9yTC6hHrqISH3NmeXynnPOnHOjnXNjU7eXnXMPpsIc59x9zrl859wY59xE59wHR63isDeG7jTkIiKNOBanzy0vL+c73/lO3Vz3qVOnMm/ePKZNm8Zrr712wL533303119//RG/fksE7peitUMuph66iDRwrE6fe9VVV9GrVy9Wr17N8uXLefzxx9m5cyezZ89m7ty5B+w7d+5cZs+efcSv3xKBO5cLoTBJDBToItJAU6fPBS/sb7rpJl555RXMjFtvvZWLL76YrVu3cvHFF1NaWko8HueBBx7gpZdeqjtFbn5+Pk899VTd8dauXcu8efN46qmnCIW8PvGwYcMYNmwYJSUl3HrrrVRXV5Oens6GDRvYsmULkydPPibtD16gmxEnAkkFuki79srNsG1p2x6z/yiYfleTm4/F6XOXL1/O2LFjCYfDB23r3bs3EyZM4NVXX2XGjBnMnTuXiy+++JhdZzV4Qy5A3NKwROMnxhERaczRPH1uffWHXY7lcAsEsYcOJCxCSD10kfbtED3po+VYnD43Pz+fxYsXk0wm64Zc6rvwwgv5/ve/z8KFC6msrGT8+PEta0wLBLKHnghFsKR66CJyoGNx+tzhw4dTUFDAbbfdVvchsXr1av70pz8B3qXlpk2bxhVXXHFMe+cQ1EC3CCEFuog0cKxOn/vwww+zbds2RowYwahRo7j66qsZOHBg3fbZs2ezePFiZs2adczaDs04fe7R0tLT5wIU/2wUC6uP49zbX23jqkSkNXT63LbV5qfPbY+SoQhh1/rLNYmIdCSBDHQXihJOxpr8kkNEpDMKZqCHI6QRJ55UoIu0N+potY2W/HcMZqCHokQtTlUs4XcpIlJPRkYGJSUlCvVWcs5RUlJCRkbGET0vkPPQCUeIEqc6nqR1PwEQkbaUm5tLUVERLb0imeyXkZFBbm7uET0nkIHuwulEUoEuIu1HJBIhLy/P7zI6rUAOuVhalCgxDbmIiNQTyEAnHPF66DH10EVEagUy0C0tnYglqI6rhy4iUiuggR4lSpwq9dBFROoEMtBDaelEiamHLiJST0ADPapZLiIiDQQz0CPetEXNchER2S+QgR5OSydqCWpiOkGXiEitQAZ6KBIFIB7TVYtERGoFMtDDEe/8BvFYtc+ViIi0HwENdK+HnlCgi4jUCWagp6UD4BToIiJ1ghnoUS/Q1UMXEdkvkIFu4VSgx/WlqIhIrUAGOuEIAMm4eugiIrUCGujel6JOgS4iUuewgW5mg8zsbTNbaWbLzeyGRvYxM7vHzNaY2RIzG390yk1J8wI9qSEXEZE6zbliURz4gXNuoZllAQvM7HXn3Ip6+0wHjk/dPgc8kLo/Oup66Ap0EZFah+2hO+e2OucWppbLgJXAcQ12mwE84TwfAT3MbECbV1srFehoyEVEpM4RjaGb2VBgHDCvwabjgE31HhdxcOhjZteYWaGZFbbqIrK1PfRErOXHEBHpYJod6GbWDXgO+J5zrrTh5kae4g5a4dxDzrkC51xBTk7OkVVaX20PPaEhFxGRWs0KdDOL4IX5U8655xvZpQgYVO9xLrCl9eU1QUMuIiIHac4sFwMeAVY6537VxG4vApelZrtMBPY657a2YZ0HSs1yCSU15CIiUqs5s1wmAZcCS81sUWrdj4HBAM65B4GXgfOBNUAF8O22L7WeNO9si6YhFxGROocNdOfcezQ+Rl5/Hwf8c1sVdVipIZdQQkMuIiK1gvlL0VQPPZRUD11EpFZAA907OVc4qR66iEitYAZ6KEyCMGH10EVE6gQz0IF4KKpAFxGpJ7CBnghFSVOgi4jUCW6gW5Q0p3noIiK1ghvo4SgRakgmDzrDgIhIpxTYQE+GokSJUZNI+l2KiEi7ENxAD6eTTozquAJdRAQCHOguFCVKnJh66CIiQIADPRlOJ91i1KiHLiICBDjQCUdJR4EuIlIrsIHu0jK8QNeQi4gIEORAD6dmuaiHLiICBDjQScvwxtDVQxcRAQIc6JbmzXJRD11ExBPcQI9kkE6NAl1EJCW4gZ6WrnnoIiL1BDbQQ5HULJdYwu9SRETahcAGukUyCJkjFtcpdEVEIMCBHo541xWN11T5XImISPsQ2EAPpQI9GVOgi4hAgAO9toeeUA9dRAQIcKCnRVM99JpKnysREWkfAhvo4VSgu3i1z5WIiLQPgQ90jaGLiHgCG+iWlg5AUj10EREgwIFOWmrIRT10EREgyIEe9nrosWoFuogIBDnQU0Mucc1yEREBmhHoZvaome0ws2VNbJ9mZnvNbFHq9pO2L7MRqUBPaMhFRASAtGbs8zhwH/DEIfb5u3Puy21SUXPVBrp+WCQiAjSjh+6cexfYdQxqOTKpMXQX0ywXERFouzH0081ssZm9Ymb5Te1kZteYWaGZFRYXF7fuFdNqA109dBERaJtAXwgMcc6NAe4F/repHZ1zDznnCpxzBTk5Oa171dpAT6iHLiICbRDozrlS51x5avllIGJmfVpd2eGkhlyIV+OcO+ovJyLS3rU60M2sv5lZanlC6pglrT3uYYXTSBImQoyqmC5DJyJy2FkuZvY0MA3oY2ZFwG1ABMA59yBwEXCdmcWBSmCWO0Zd5ng4gy7xasqr42RGw8fiJUVE2q3DBrpzbvZhtt+HN63xmItHs+heU0F5dZycrHQ/ShARaTeC+0tRIB7tQTb72Fcd97sUERHfBTrQXUY22VZOuQJdRCTggZ7Zk2z2UV6lQBcRCXSghzJ70sPK2VejQBcRCXSgh7v2Ipt9lKmHLiLSrJNztVuRrj2JWoyqinK/SxER8V2ge+iRbr0BSFTs9rkSERH/BTrQLbMHAImK9ncySBGRYy3QgU5mT+++co+/dYiItAMBD3Svhx6uUqCLiAQ80L0eulXv9bkQERH/BTvQM7weulXqS1ERkWAHenp3koRAQy4iIgEP9FCImrQsorFSqmIJv6sREfFVsAMdiKdn08P2sWVPpd+liIj4KvCBTkYPelLGlj26WLSIdG6BD/RQj0EcZzvVQxeRTi/wgZ7e7wQG2w627C7zuxQREV8FPtDDOSOIWILq4vV+lyIi4qvABzq9RwBgu9b5XIiIiL+CH+i9hgOQWbbB3zpERHwW/EDv2oeqcFd6Vn5GTTzpdzUiIr4JfqCbUZWVx2C2smqbvhgVkc4r+IEORPoeT55tY1GRTgEgIp1Xhwj0LoPHMShUzLp1a/wuRUTENx0i0G3YVAAim973uRIREf90iECn/2iqwlkMK19IaVXM72pERHzRMQI9FKZi4EROt+X8/dOdflcjIuKLjhHoQI+RZzMktIPChfP8LkVExBcdJtBDp3yFBGFy1z9LZY3OjS4inc9hA93MHjWzHWa2rIntZmb3mNkaM1tiZuPbvsxmyOrH7kHncCHv8OayTb6UICLip+b00B8HzjvE9unA8anbNcADrS+rZXpOuYbeVsbaNx/FOedXGSIivjhsoDvn3gV2HWKXGcATzvMR0MPMBrRVgUcifPzZ7MwezcXlTzBvlXrpItK5tMUY+nFA/fQsSq07iJldY2aFZlZYXFzcBi990AuQNeMu+ttuNv/pP0gm1UsXkc6jLQLdGlnXaJI65x5yzhU45wpycnLa4KUPlj5sEhsGf5WZFc/xzl+fPyqvISLSHrVFoBcBg+o9zgW2tMFxW2zIJb9mW9pAxnz0PbZt/MTPUkREjpm2CPQXgctSs10mAnudc1vb4LgtZulZMOtp0lyC5BNfoWaXxtNFpONrzrTFp4EPgRPNrMjMrjSza83s2tQuLwPrgDXAHOD6o1btERh4/BiWnDGHrPguyh74Am6XLlEnIh2b+TW9r6CgwBUWFh7113n6hReYvuifCad3I+vKF6Bf/lF/TRGRo8XMFjjnChrb1mF+KdqUWRdeyMPD7qGiuprYQ+fAJy/7XZKIyFHR4QPdzPiXS2ZyV+4DrIj1x839BrzzX5DU6QFEpGPp8IEOkJ4W5q5vn8v9Q+/lhcQkeOdOeGIGlPr63a2ISJvqFIEOXqjfc9npvDridn4Y+w41n83HPTgJVr/ud2kiIm2i0wQ6eKH+wKUFZJ52GdMr/y+b49nw1EXw2i0Qr/G7PBGRVulUgQ4QDhk/nZHPxdPP5uzSn/BK5gXw4X0w5yzY1ugJJUVEAqHTBTp4X5ReM3U4v5j9OW4ou4Qb035ErHQbPDQN/v5LSMT9LlFE5Ih1ykCvdcGYgTx37ef5IG0Ck8vuYFO/s+DNn8Jj58HONX6XJyJyRDp1oAOMys3mxe9OYsTQIUxZ/y3mDr4dt3M1PDgZPnoQkkm/SxQRaZZOH+gAvbul87tvT+A7Zwzj5k9P4Mqu91KZ+3l49d/h8fOh+FO/SxQROSwFekpaOMSPpp/M/ZeM5x8705mw4RoWn3on7FgJD06Cd/8bEjG/yxQRaZICvYHzRw3g5X+dwrCcLGa8P5Q78h4nfvx0eOs/4bdnwOYFfpcoItIoBXojBvfuwrPXns61ZwxnzscVnLflSj774sNQuQsePsebt15d7neZIiIHUKA3IRIOcfP0k3jyygnsrYxx9ktdmTP6aZLjLvPmrf9mAix/AXQxahFpJxTohzHl+BxevWEKXxjZjzve3MLMTV+naOb/Qpde8MfL4ckL9aWpiLQLCvRm6N0tnfsvOZV7Z49jY8k+zvpjFXNOeoTkeT+HzR/DA5+HN26Hmn1+lyoinZgC/QhcMGYgf/23qZxxQg53vLqGr308io3f+BuM/jq89z9w32mw5A+auy4ivlCgH6G+WRk8dOmp3H3xWFZvL+MLcz7hnqx/o+Zbr0DXPvD81fDw2bDxA79LFZFORoHeAmbGheOO443vn8EXR/bjV69/yvTna/jonOfgwgehbBs8Nh3mXgIla/0uV0Q6CQV6K/TtnsF93xjP498+jZpEkllz/sGNq0ey68oP4cxbYe3b8JvPwas/gopdfpcrIh1ch79I9LFSWZPgnrdWM+fddWRlpPGj6Sdz0YkRQu/cCR8/Cend4Yyb4LSrIS3qd7kiElCHuki0Ar2NrdpWxo9fWMqCjbsZM6gHt18wknHpW+Cvt8Lat6DHEDjzxzDqaxAK+12uiATMoQJdQy5t7MT+WfzxO6fzq6+PYeueSmbe/wE/+FucHTOehm8+BxnZ8MJ3vLM5fvKyfpgkIm1GgX4UhELGV8bn8tYPp3HdtOH8efEWzvzFO/x2cx41V74NFz0G8WqYOxse+SJseM/vkkWkA9CQyzGwYec+/vOlFbyxcgd5fbryf758MmeO6Ikt/j28cxeUbYUR58DZP4EBY/wuV0TaMY2htxPvrNrBT/+ygnXF+5g0ojc/mn4yp/SNwj/mwHu/gsrdkD8TzrgZ+p7kd7ki0g4p0NuRWCLJ7+d9xt1vfMqeyhgzxx7HD889kYEZNfDBvfDRA94pBE75Kpzx75Bzgt8li0g7okBvh0qrYtz/9loefX89BlwxOY/rpg2ne6IUPrwX5j0E8Uo45SIv2PuM8LtkEWkHFOjtWNHuCn7510954ePN9Ooa5XvnHM/sCYOJVO2C938N8x+GeBWM+ro3j733cL9LFhEfKdADYGnRXu58eSUfrithWJ+u3HTeSZyb3w/btxPevxvmPwKJGhh9MUz5gXrsIp2UAj0gnHO8vWoHd778CWt2lDN+cA9uOu8kJg7rDWXbvWAvfNQL9vyZXrD3y/e7bBE5hlr9wyIzO8/MVpnZGjO7uZHtl5tZsZktSt2uam3RnZGZcdZJ/Xj1hin87Cuj2LynklkPfcRlj/6DZaUZcN7P4HtL4fP/Ap++5p2H/elv6DqnIgI0o4duZmHgU+ALQBEwH5jtnFtRb5/LgQLn3Heb+8LqoR9eVSzBEx9u4P531rKnIsaXRg/gB184gWE53byTfc37Lcx7EKr2wLAzYeqNMHSS32WLyFHU2h76BGCNc26dc64GmAvMaMsCpXEZkTDXTB3Ouzedyb+eNYK3P9nBF/7nXW5+bglbY5lw5o/g35bBOf8B25fB4+fDo+fB6jd0SgGRTqg5gX4csKne46LUuoa+amZLzOxZMxvU2IHM7BozKzSzwuLi4haU2zl1z4jw/S+eyLs3ncmlE4fw/MLNnPHf73DHSyvYFU+Hyd+DG5bA9J/Dns/gqa/Cb6fC0mchEfO7fBE5Rpoz5PI14Fzn3FWpx5cCE5xz/1Jvn95AuXOu2syuBb7unDvrUMfVkEvLFe2u4O43VvP8wiK6RNO4/PNDuWpKHj26RCFeA0vmwvv3QMlqyB4EE6+D8ZdBepbfpYtIK7VqlouZnQ7c7pw7N/X4RwDOuZ81sX8Y2OWcyz7UcRXorbd6exl3v7mal5ZspVt6Gt+eNJSrJg8ju0vEu67p6te8X59ufB/Ss6Hg2/C5a6H7AL9LF5EWam2gp+F9KXo2sBnvS9FvOOeW19tngHNua2p5JvDvzrmJhzquAr3trNpWxq/f/JSXl24jKz2Nb0/O48rJeWRnRrwdigq9YF/5IljYu6j16d+FfiP9LVxEjlir56Gb2fnA3UAYeNQ5d4eZ/RQodM69aGY/A/4JiAO7gOucc58c6pgK9La3cmspv35jNa8u30ZWRhpXTs7jisl5dM9IBfuudd65Yj7+fxCr8M7w+Pl/hbypYOZv8SLSLPphUSezfMte7nlzNa8t3073DG+M/fJJefTqmrr0XcUu75en//gt7CuGvvkw4Wqv5x7t6m/xInJICvROatnmvdz7lhfsmZEwsycM5uqpeQzIzvR2iFXB0j94JwLbvtS7mtK4S+G0q6BXnr/Fi0ijFOid3OrtZTzwt7X8adEWQgZfGZfLtdOGk9cn1Rt3Dj77yOuxr3gRXBJOONfrtQ87C0K6sJVIe6FAFwA27apgzt/X8cz8TcQSSaaPGsD104aTP7DehKTSLVD4GCx4zBuO6ZkHp34Lxl4C3fr6V7yIAAp0aaC4rJpH31/Pkx9upLw6ztQTcrhqch5Tju+D1X45Gq/2eusLHoeN70EoDU48H0693DvNgHrtIr5QoEuj9lbG+H8fbeTxDzZQXFbNCf26ceXkPGaMPY6MSHj/jsWfwsLfwaLfQ+Uu6DHE+6HSuG9CVn//GiDSCSnQ5ZCq4wn+sngrD7+3npVbS+ndNcqlpw/hmxOH0Kdb+v4d49Ww8s9euK9/15vTfsK53jnaTzgPIhn+NUKkk1CgS7M45/hwbQmPvLeeNz/ZQTQtxIwxA7ns9KGMym3ww9+StV6wL34GyrdBencY+U9euA+ZrCEZkaNEgS5HbG1xOY++t57nF26mMpZgTG4235w4hAvGDDxwOCaZ8HrrS//ojbnXlEHWQBh1kTevvd8p+tGSSBtSoEuLlVbFeGHhZp78aCNrdpSTnRnholNzueRzg73zstcXq4RVr8CSP8Ca1yEZh5yTvZ77SV+C/qMV7iKtpECXVnPOMW/9Lp78aCOvLdtGPOmYNKI3Xy8YxLn5/Q/stQPsK4EVL8Cy5+GzD7257dmDvWA/6Usw+HQIp/nTGJEAU6BLm9pRVsUz/9jE3Pmb2LynkqyMNC4YM5CLTs1l3KAe+6c+1tq30+u5f/ISrH0LEtWQ2QtOnA4jzoa8M6BrH38aIxIwCnQ5KpJJx0frS3i2sIiXl22lKpZkeE5XvnpqLheMHsigXl0OflJ1Oax90wv3T1+Fqr3e+v6jYfiZ3hz3wRMhknlsGyMSEAp0OerKqmK8vHQrzy4oYv6G3QCMG9yDC0YP5EujB9CveyNTGpMJ2PIxrHsb1r4Dm+ZBMgZpGV6oD/485BbAcadCZo9j2yCRdkqBLsfUpl0V/GXJVv68eAsrtpZiBp/L68WXRg/kiyP7NR7u4PXeN34A697xbjtWAKl/n31OhNzTvIDPPQ36ngyhcOPHEenAFOjimzU7yvnLki28uHgL64r3ATAmN5tzTu7HOSP7cVL/rIPH3GtVlcKWhVA037tIR9F8qCjxtkW7wcBxcNx46DsSck6CnBM1VCMdngJdfOecY82Ocv66YjtvrNzOx5/tASC3ZyZnn9SXKcfn8LlhvciqvRhH4weB3ev3h3vRfNi2zBumAcCg51Cv955z0v77nkO8UwOLdAAKdGl3dpRV8dbKHby+Yjvvr91JVSxJWsgYO6gHk4/vw+QRfRgzqAeR8GF+cZqIeVdi2rESij/Zf1+yxpsHXyuaBdm5Td+yBkJa9Og2WqQNKNClXauOJ1iwcTfvrd7J+2t2smTzXpyDLtEwYwf1oGBITwqG9mLc4B6H7sHXF6/xQn3nKthbdPCtYmeDJ5g3dTKzl/cFbGZP75ZRu9zYup5ez1/z6eUYUqBLoOypqOGDtSV8tK6Ewg27+WRbKUkHIYMT+3dn/OAe5A/M5pTjunNCv6yDf9TUHLFK2LsZ9m6C0s1eyJdthcrdULln/33VHqguPfSxolne2H0kA9JS95EuEI56f0EkavbPve8xyLtSVLzKe056lrdvKM3bPxz1zoNjtbew9+VvOOrN/klL927h9HrLUe/5ofD+/S3U4HEz1utXvIGgQJdAK6uKsWjTHgo37GbBxt0sLtpDWZU3nJIWMkb07Ub+wGxOHpDFiL7dGNG3GwOzMwmF2iigEjFvvnxd0O/2gr52uXIPxCu9D4lYpRfWsUrveeFI6haF8h3eh0Yk0wvnWKX3YRGvToV+TdvU21K1HyBpGfs/nNK7Qbd+3i0tPfXBkbb/A6TRx2Hv+w6XBFyDZVL3SW/9oZYjXb3TM9f/sDKrt5z6cCL1Ptd9IDXyuO6fQjP2Pei5R7Bv3ePUcuVu732NdvG+yI92hexBkNG9mW/Kwa170CUAAAhbSURBVBTo0qE459i0q5JlW/ayfMtelm8pZdnmUnaWV9ftkxkJMyynKyP6diOvT1dye3Yht2cmuT0z6d89g7TDjc37wTlv3N8lvVsykbqPe6EQr/KGkhLV3odAvNpbl6jx9kkmwCUgmUzdJxrcN2N97WvFKrzjV5dB2TbYt8N77WR8/2vVLccO37ZDSoWmhfYvY147O6KvPuKdvK6FDhXoGvyTwDEzBvfuwuDeXTh/1IC69SXl1awt3seaHeWs2VHO2uJyCjfs5sXFW6jfbwmHjAHZGeT2zGRgdiY53dPpm5VBTlY6Od3S6ds9nT5d08nKSGu7Xn7zGub15oMomawX8PF6Qzh2iOX6vd9GxKq8yyDWffi4esv1PoSAut8r1L3PtY9dg+WG2w73uAXPrV9DRrb3l01NBcT2Qc0+74dyR4kCXTqM3t3S6d0tnQl5vQ5YXx1PsHVPFUW7KynaXXHA/UfrSthZXkNNInnQ8UIG3TMj9MiMkJ0ZIbtLlOx6j7ukh8mMhOkSDZMRCdMlmkZmJExm1FtXuy09EiYSNkJmVMeSYJARCRENh5qegx80oRCEokAbzhSKZHjfOUizKdClw0tPCzO0T1eG9una6HbnHHsrYxSXVbOjrJrismp2lleztzLG3soYeypi7Ektf1ayjz2VMUorYyRbOVppBulpITIiYTLSwmREvOVoWoiQGeGQETYjFPL+qjhwnXcfDtUuQzgUItxg37rn1C1zwPNDoYbHhHA4RNgMh6v7MjoaDhFNC5Geqi2Uqsss9VwzQuY9DhmEQvUfp9aZeUPgB+wPsH9fY/9+4B3HUv+tQla77G2vv69hXuef/TUYVvdHgDV4DUstdzQKdOn0zIweXaL06BLl+H5ZzXqOc47qeJKKmgSVsQSVNXEqa5JU1MRTj731FTUJqmIJEklHPOnqZuRUxRJUxxJUxZNUxRKpm7dck0iSSDqSznn3SYg1WHfAdgfxZJJkEm+bcyRT997za9dRt048B35QHPyhUrtc+73q/g8Y7wMCaj+kDvzQgNRkJeyA1yC1POu0QVw1ZVibt0eBLtICZub1rFsyZbIdOCDw631wJOp9YIRSQ93OQU08SXU8SU08SdI5nPP29Za9D5Vk6sOl7nHd9tpl6vZPJKn7C6B2YkbtvrX7106IqV12ByzX7ps6Rv111L7m/mXnmlh30HH3r08mD9wO9V9v//Fgfxvrv0ZTteM48Fq9bUiBLtIJhUJGCCOgn0fShHY4d0tERFpCgS4i0kEo0EVEOohmBbqZnWdmq8xsjZnd3Mj2dDN7JrV9npkNbetCRUTk0A4b6GYWBn4DTAdGArPNbGSD3a4EdjvnRgD/A/xXWxcqIiKH1pwe+gRgjXNunXOuBpgLzGiwzwzgd6nlZ4GzrSPO2hcRaceaE+jHAZvqPS5KrWt0H+dcHNgL9G54IDO7xswKzaywuLi4ZRWLiEijmhPojfW0G/7UrDn74Jx7yDlX4JwryMnJaU59IiLSTM35YVERUP8MObnAlib2KTKzNCAb2HWogy5YsGCnmW08glrr6wM0vORMR9aZ2tuZ2gqdq72dqa1w9No7pKkNzQn0+cDxZpYHbAZmAd9osM+LwLeAD4GLgLfcYU607pxrcRfdzAqbOh9wR9SZ2tuZ2gqdq72dqa3gT3sPG+jOubiZfRd4DQgDjzrnlpvZT4FC59yLwCPAk2a2Bq9nPutoFi0iIgdr1rlcnHMvAy83WPeTestVwNfatjQRETkSQf2l6EN+F3CMdab2dqa2Qudqb2dqK/jQXt+uKSoiIm0rqD10ERFpQIEuItJBBC7QD3eisKAzsw1mttTMFplZYWpdLzN73cxWp+57+l1nS5nZo2a2w8yW1VvXaPvMc0/qvV5iZuP9q/zINdHW281sc+r9XWRm59fb9qNUW1eZ2bn+VN1yZjbIzN42s5VmttzMbkit73Dv7yHa6u/761KXhArCDW/a5FpgGN7lxRcDI/2uq43buAHo02Ddz4GbU8s3A//ld52taN9UYDyw7HDtA84HXsH7JfJEYJ7f9bdBW28HftjIviNT/57TgbzUv/Ow3204wvYOAManlrOAT1Pt6nDv7yHa6uv7G7QeenNOFNYR1T/52e+AC32spVWcc+9y8K+Im2rfDOAJ5/kI6GFmA45Npa3XRFubMgOY65yrds6tB9bg/XsPDOfcVufcwtRyGbAS7zxPHe79PURbm3JM3t+gBXpzThQWdA74q5ktMLNrUuv6Oee2gvcPCejrW3VHR1Pt66jv93dTQwyP1hs+61BtTV0TYRwwjw7+/jZoK/j4/gYt0Jt1ErCAm+ScG493/vl/NrOpfhfko474fj8ADAfGAluBX6bWd5i2mlk34Dnge8650kPt2si6QLW5kbb6+v4GLdCbc6KwQHPObUnd7wBewPuzbHvtn6Kp+x3+VXhUNNW+Dvd+O+e2O+cSzrkkMIf9f3Z3iLaaWQQv4J5yzj2fWt0h39/G2ur3+xu0QK87UZiZRfHOGfOizzW1GTPramZZtcvAF4Fl7D/5Gan7P/lT4VHTVPteBC5LzYaYCOyt/dM9qBqMEc/Ee3/Ba+ss8y7nmAccD/zjWNfXGqmL2jwCrHTO/arepg73/jbVVt/fX7+/LW7Bt8vn432jvBa4xe962rhtw/C+CV8MLK9tH97FQt4EVqfue/ldayva+DTen6IxvF7LlU21D+/P1N+k3uulQIHf9bdBW59MtWVJ6n/yAfX2vyXV1lXAdL/rb0F7J+MNIywBFqVu53fE9/cQbfX1/dVP/0VEOoigDbmIiEgTFOgiIh2EAl1EpINQoIuIdBAKdBGRDkKBLiLSQSjQRUQ6iP8PYPQHwWyBZIAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "x=np.linspace(0,len(cost_CV),num=len(cost))\n",
    "line1, = ax.plot(x, np.array(cost),label='Cost train')\n",
    "x=np.arange(len(cost_CV))\n",
    "line2, = ax.plot(x, np.array(cost_CV),label='Cost CV')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Timing at each step\n",
    "This section measures the time occupied in each task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time in feedforwards:  0.4925177574157715\n"
     ]
    }
   ],
   "source": [
    "#Time of feedforward\n",
    "start = time.time()\n",
    "number=10\n",
    "for i in range(number):\n",
    "    A,Z=feedforward_model(parameters,typeN,x_train)\n",
    "end = time.time()\n",
    "print(\"Time in feedforwards: \", (end - start)/number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time in backprop:  0.5297061920166015\n"
     ]
    }
   ],
   "source": [
    "#Time of backprop\n",
    "start = time.time()\n",
    "number=10\n",
    "for i in range(number):\n",
    "    grads=backprop_model(parameters,typeN,x_train,Y_train,A,Z)\n",
    "end = time.time()\n",
    "print(\"Time in backprop: \", (end - start)/number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time in regularization grad:  0.0026728463172912597\n"
     ]
    }
   ],
   "source": [
    "#Time of regularization grad\n",
    "start = time.time()\n",
    "number=100\n",
    "for i in range(number):\n",
    "    grada=RegularizationL2(parameters,grads,1,m_train)\n",
    "end = time.time()\n",
    "print(\"Time in regularization grad: \", (end - start)/number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time in cost and accuracy:  0.020734241008758546\n"
     ]
    }
   ],
   "source": [
    "#Time of cost and accuracy\n",
    "start = time.time()\n",
    "number=100\n",
    "for i in range(number):\n",
    "    cost=sigmoid_cost(A[L],Y_train)+RegularizationL2_cost(parameters,1,m_train)\n",
    "    acc=accuracy(Y2y(A[L]),y_train)\n",
    "end = time.time()\n",
    "print(\"Time in cost and accuracy: \", (end - start)/number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time in backprop:  0.011056232452392577\n"
     ]
    }
   ],
   "source": [
    "# Time at ADAM\n",
    "start = time.time()\n",
    "number=20\n",
    "for i in range(number):\n",
    "    grads1,V1,S1,counter=ADAM_grad(grads,V,S,n_x,n_h,0.9,0.999,1)\n",
    "end = time.time()\n",
    "print(\"Time in backprop: \", (end - start)/number)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient checking\n",
    "Gradient checking is used to assure backprop is right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum difference between vectors is: 0.21034410^(-7)\n"
     ]
    }
   ],
   "source": [
    "def Wb2vector(parameters,name):\n",
    "    ## This function flattens the parameter dictionary in one column vector\n",
    "    ## INPUTS:\n",
    "    ##### parameters: parameters of all the layers (\"W_\" and \"b_\")\n",
    "    ##### name: list that contains the name of the dictionary keys: [\"W\",\"b\"] or[\"dW\",\"db\"]\n",
    "    ## OUTPUTS\n",
    "    ##### vector: one-column vector with parameters flattened\n",
    "    L=len(parameters)//2\n",
    "\n",
    "    vector=parameters[name[0]+str(1)].reshape(-1,1)\n",
    "    vector=np.vstack((vector,parameters[name[1]+str(1)].reshape(-1,1)))\n",
    "    if L>1:\n",
    "        for l in range(2,L+1):\n",
    "            vector=np.vstack((vector,parameters[name[0]+str(l)].reshape(-1,1)))\n",
    "            vector=np.vstack((vector,parameters[name[1]+str(l)].reshape(-1,1)))\n",
    "    return vector\n",
    "\n",
    "def vector2Wb(parameters, vector,name):\n",
    "    ## This function undo Wb2vector\n",
    "    ## INPUTS:\n",
    "    ##### parameters: parameters of all the layers (\"W_\" and \"b_\")\n",
    "    ##### vector: one-column vector with parameters flattened\n",
    "    ##### name: list that contains the name of the dictionary keys: [\"W\",\"b\"] or[\"dW\",\"db\"]\n",
    "    ## OUTPUTS\n",
    "    ##### parameters: parameters of all the layers (\"W_\" and \"b_\")\n",
    "    L=len(parameters)//2\n",
    "    parameters_aux={}\n",
    "    cont=0\n",
    "    nf=parameters[name[0]+str(1)].shape[0]\n",
    "    nc=parameters[name[0]+str(1)].shape[1]\n",
    "    parameters_aux[name[0]+str(1)]=vector[cont:cont+nf*nc].reshape(nf,nc)\n",
    "    cont+=nf*nc\n",
    "    nf=parameters[name[1]+str(1)].shape[0]\n",
    "    nc=parameters[name[1]+str(1)].shape[1]\n",
    "    parameters_aux[name[1]+str(1)]=vector[cont:cont+nf*nc].reshape(nf,nc)\n",
    "    cont+=nf*nc\n",
    "    if L>1:\n",
    "        for l in range(2,L+1):\n",
    "            nf=parameters[name[0]+str(l)].shape[0]\n",
    "            nc=parameters[name[0]+str(l)].shape[1]\n",
    "            parameters_aux[name[0]+str(l)]=vector[cont:cont+nf*nc].reshape(nf,nc)\n",
    "            cont+=nf*nc\n",
    "            nf=parameters[name[1]+str(l)].shape[0]\n",
    "            nc=parameters[name[1]+str(l)].shape[1]\n",
    "            parameters_aux[name[1]+str(l)]=vector[cont:cont+nf*nc].reshape(nf,nc)\n",
    "            cont+=nf*nc\n",
    "    return parameters_aux\n",
    "\n",
    "# Creation of dummy parameters, xx and yy\n",
    "parameters={}\n",
    "parameters[\"W1\"]=np.random.rand(4,5)\n",
    "parameters[\"W2\"]=np.random.rand(3,4)\n",
    "parameters[\"W3\"]=np.random.rand(10,3)\n",
    "parameters[\"b1\"]=np.random.rand(4,1)\n",
    "parameters[\"b2\"]=np.random.rand(3,1)\n",
    "parameters[\"b3\"]=np.random.rand(10,1)\n",
    "xx=np.random.rand(5,10)\n",
    "yy_aux=np.random.randint(0,10,(1,10))\n",
    "yy=np.zeros((10,yy_aux.shape[1]))\n",
    "yy[yy_aux,range(yy_aux.shape[1])]=1\n",
    "\n",
    "lambda_regularization=1\n",
    "\n",
    "# Cost function\n",
    "loss=loss_function(typeN) \n",
    "\n",
    "# feedforward+backprop\n",
    "m_xx=xx.shape[1]\n",
    "A,Z=feedforward_model(parameters,typeN,xx)\n",
    "costs_f=loss(A[L],yy)+RegularizationL2_cost(parameters,lambda_regularization,m_xx)\n",
    "grads=backprop_model(parameters,typeN,xx,yy,A,Z)\n",
    "grads=RegularizationL2(parameters,grads,lambda_regularization,m_xx)\n",
    "grads_V=Wb2vector(grads,[\"dW\",\"db\"])\n",
    "\n",
    "\n",
    "# Gradient checking\n",
    "V=Wb2vector(parameters,[\"W\",\"b\"])\n",
    "d=V.shape[0]\n",
    "grad_approx=np.zeros((d,1))\n",
    "epsilon=0.000001\n",
    "for i in range(d):\n",
    "    aux=np.copy(V)\n",
    "    aux[i]=aux[i]+epsilon\n",
    "    parameters_aux=vector2Wb(parameters, aux,[\"W\",\"b\"])\n",
    "    A,Z=feedforward_model(parameters_aux,typeN,xx)\n",
    "    cost_aux_P=loss(A[L],yy)+RegularizationL2_cost(parameters_aux,lambda_regularization,m_xx)\n",
    "    aux=np.copy(V)\n",
    "    aux[i]=aux[i]-epsilon\n",
    "    parameters_aux=vector2Wb(parameters, aux,[\"W\",\"b\"])\n",
    "    A,Z=feedforward_model(parameters_aux,typeN,xx)\n",
    "    cost_aux_N=loss(A[L],yy)+RegularizationL2_cost(parameters_aux,lambda_regularization,m_xx)\n",
    "    grad_approx[i]=(cost_aux_P-cost_aux_N)/(2*epsilon)\n",
    "    \n",
    "maximum_difference=((np.abs(grad_approx-grads_V))/grads_V).max()\n",
    "print(\"Maximum difference between vectors is: %f10^(-7)\" %(maximum_difference*1e7))\n",
    "#print(np.hstack((grads_V.reshape(-1,1),grad_approx.reshape(-1,1))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
